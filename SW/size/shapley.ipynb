{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data import get_data, get_processed_data\n",
    "from train_test import train, output_to_accu, test\n",
    "from helpers import plot_cm, count_parameters\n",
    "from models import MLP, ConvNet, LSTM\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "import shap\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, target_prices, _, features = get_data()\n",
    "\n",
    "input_period = 4\n",
    "input_period_reg = 10\n",
    "training_window = 5\n",
    "\n",
    "X, _, y, _ = get_processed_data(features, target_prices, input_period, input_period_reg, training_window)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of train sample : 1046\n",
      "Number of test sample : 262\n"
     ]
    }
   ],
   "source": [
    "train_indices, test_indices, _, _ = train_test_split(range(len(y)), y, test_size=0.2, shuffle=False)\n",
    "X_train, y_train, X_test, y_test = X[train_indices], y[train_indices], X[test_indices], y[test_indices]\n",
    "print('Number of train sample :', len(X_train))\n",
    "print('Number of test sample :', len(X_test))\n",
    "\n",
    "X_mean = X_train.mean(dim=[0, 1], keepdim=True)\n",
    "X_std = X_train.std(dim=[0, 1], keepdim=True)\n",
    "X_train = X_train.sub_(X_mean).div_(X_std)\n",
    "X_test = X_test.sub_(X_mean).div_(X_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:01<00:00, 13.33it/s]\n"
     ]
    }
   ],
   "source": [
    "eta = 1e-3\n",
    "weight_decay = 1e-4\n",
    "dropout = 0.2\n",
    "batch_size = 30\n",
    "nb_epochs = 20\n",
    "\n",
    "verbose = 3\n",
    "\n",
    "model_name = 'MLP'\n",
    "\n",
    "dim1, dim2 = X.size(1), X.size(2)\n",
    "\n",
    "if model_name == 'MLP':\n",
    "    model = MLP(dim1, dim2, pdrop=dropout)\n",
    "\n",
    "elif model_name == 'ConvNet':\n",
    "    model = ConvNet(dim1, dim2, pdrop=dropout)\n",
    "\n",
    "elif model_name == 'LSTM':\n",
    "    model = LSTM(input_size=dim2, output_size=y.size(1), device=device, pdrop=dropout)\n",
    "    \n",
    "train(model, X_train, y_train, nb_epochs=nb_epochs, device=device, X_test=X_test, y_test=y_test, \n",
    "      batch_size=batch_size, eta=eta, weight_decay=weight_decay, verbose=verbose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n"
     ]
    }
   ],
   "source": [
    "explainer = shap.DeepExplainer(model, X_test.narrow(0, 0, 100))\n",
    "shap_values = explainer.shap_values(X_test.narrow(0, 100, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
